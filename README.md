# ml-linear-regression-practice


# Building and Evaluating a Linear Regression Model Using Machine Learning

## ğŸ“Œ Project Overview

This project demonstrates an end-to-end **Supervised Machine Learning** workflow using **Linear Regression** to build a prediction model. The focus is on applying proper **data preprocessing, feature engineering, model training, evaluation, and performance analysis** using real-worldâ€“style data.

The project is implemented in **JupyterLab** using **Python** and popular ML libraries.

---

## ğŸ› ï¸ Tools & Libraries Used

* **Python**
* **Pandas** â€“ data manipulation
* **NumPy** â€“ numerical operations
* **Seaborn & Matplotlib** â€“ data visualization
* **Scikit-learn** â€“ machine learning (modeling, preprocessing, evaluation)

---

## ğŸ“‚ Project Workflow

### 1ï¸âƒ£ Data Loading & Exploration

* Loaded the dataset using **Pandas**
* Performed basic data exploration (shape, info, null values)
* Visualized relationships using **Seaborn**

---

### 2ï¸âƒ£ Feature & Target Separation

* Separated **independent features (X)** and **target variable (y)**

```python
X = data.drop(columns=["target"])
y = data["target"]
```

---

### 3ï¸âƒ£ Feature Engineering

#### ğŸ”¹ One-Hot Encoding

* Applied **One-Hot Encoding** for categorical variables (e.g., region)
* Used `drop_first=True` to avoid the **dummy variable trap**

```python
pd.get_dummies(X, columns=["region"], drop_first=True)
```

#### ğŸ”¹ Binary Encoding

* Converted binary categorical features into numerical format (0/1)

#### ğŸ”¹ Interaction Features

* Created interaction features to capture combined effects between variables

```python
X["age_smoker"] = X["age"] * X["smoker"]
```

---

### 4ï¸âƒ£ Feature Scaling

* Applied **Normalization** and **Standardization** for numeric features
* Especially important where values were on different scales (e.g., salary in lakhs)

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

### 5ï¸âƒ£ Feature Selection

* Selected important features such as:

  * Score
  * Study Hours
* Reduced noise and improved model performance

---

### 6ï¸âƒ£ Train-Test Split

* Split data into training and testing sets

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### 7ï¸âƒ£ Model Training

* Trained the model using **Linear Regression**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

---

### 8ï¸âƒ£ Prediction

* Predicted target values using the trained model

```python
y_pred = model.predict(X_test)
```

---

### 9ï¸âƒ£ Model Evaluation

#### ğŸ“Š RÂ² Score

* Measured how well the model explains variance in the target

```python
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
```

#### ğŸ“Š Adjusted RÂ²

* Used to account for the number of features in the model

```python
adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)
```

---

### ğŸ”Ÿ Underfitting & Overfitting Analysis

* Compared **training vs testing performance**
* Evaluated whether the model was:

  * Underfitting (too simple)
  * Overfitting (too complex)

---

## âœ… Key Learnings

* Importance of **feature engineering** in Linear Regression
* Handling categorical variables correctly
* Role of **scaling** in model stability
* Understanding **RÂ² vs Adjusted RÂ²**
* Detecting **underfitting and overfitting**

---

## ğŸš€ Conclusion

This project strengthened my understanding of **machine learning fundamentals** and how Linear Regression works on real-world data. It highlights the complete ML pipelineâ€”from raw data to model evaluationâ€”using best practices.

---

## ğŸ“Œ Future Improvements

* Try **Regularization techniques** (Ridge, Lasso)
* Perform **cross-validation**
* Experiment with additional interaction features

---

â­ If you find this project helpful, feel free to star the repository!.
